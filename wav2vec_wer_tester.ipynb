{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from match_em import analysis\n",
    "\n",
    "results = analysis.compute_mistakes('frå neste veke av vart altså', 'fra neste veka var altså')\n",
    "test_lines = [l.strip() for l in results['final_print'].split('\\n') if l]\n",
    "\n",
    "truth_lines = [l.strip() for l in '''--- UNK_ID (WER: 60.0, compounds created: 1, compounds broken up: 0)---\n",
    "  f | r | å    ||  neste  ||   v | e | k | e |   | a | v    ||   v | a | r | t    ||  altså  || \n",
    "  f | r | a    ||  neste  ||   v | e | k |   |   | a |      ||   v | a | r |      ||  altså  || \n",
    "    |   | S    ||         ||     |   |   | D | D |   | D    ||     |   |   | D    ||         || \n",
    "    '''.split('\\n') if l.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(len(test_lines)):\n",
    "    if test_lines[l] != truth_lines[l]:\n",
    "        print('''\n",
    "        ---\n",
    "        -{}-\n",
    "        -{}-\n",
    "        ---\n",
    "\n",
    "        '''.format(truth_lines[l], test_lines[l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lines == truth_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- UNK_ID (WER: 66.67, compounds created: 0, compounds broken up: 0)---\n",
      "  f | r | å    ||  neste  ||   v | e | k | e    ||   a | v    ||   v | a | r | t    ||  altså  || \n",
      "  f | r | a    ||  neste  ||   v | e | k | a    ||     |      ||   v | a | r |      ||  altså  || \n",
      "    |   | S    ||         ||     |   |   | S    ||   D | D    ||     |   |   | D    ||         || \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = analysis.compute_mistakes('frå neste veke av vart altså', 'fra neste veka var altså', distance_method='weighted_manual')\n",
    "print(results['final_print'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('replace', 2, 2), ('replace', 5, 5), ('delete', 8, 8), ('replace', 9, 8), ('replace', 10, 9), ('replace', 13, 12)]\n",
      "\n",
      "[('sub', 2, 2), ('sub', 5, 5), ('sub', 8, 8), ('delete', 9, 8), ('sub', 10, 9), ('sub', 13, 12)]\n"
     ]
    }
   ],
   "source": [
    "from match_em.distances import distance\n",
    "\n",
    "dist = distance(\n",
    "    'sau hadde dei hatt på heimgarden og sau ville han halda på med vidare',\n",
    "    'sau hadde de hatt på heimgaren og sau villand holda på med videre'\n",
    ")\n",
    "\n",
    "print(dist.get_levenshtein_editops())\n",
    "print()\n",
    "print(dist.get_weighted_editops())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[('sub', 2, 2), ('sub', 5, 5), ('delete', 8, 8), ('sub', 9, 8),   ('sub', 10, 9), ('sub', 13, 12)]\n",
    "\n",
    "[('sub', 2, 2), ('sub', 5, 5),  ('sub', 8, 8),  ('delete', 8, 9), ('sub', 9, 10), ('sub', 12, 13)]\n",
    "\n",
    "\n",
    "\n",
    "sau |  hadde |  dei |  hatt |  på |  heimgarden |  og |  sau |   ville  |    han   |  halda |  på |  med |  vidare\n",
    "\n",
    "sau |  hadde |  de  |  hatt |  på |  heimgaren  |  og |  sau |          |  villand |  holda |  på |  med |  videre\n",
    " 0  |    1   |   2  |   3   |  4  |      5      |  6  |   7  |    8     |     9    |    10  |  11 |  12  |   13   \n",
    "             | sub  |             |     sub     |     |      |    del   |     sub  |   sub  |     |      |   sub\n",
    "\n",
    "sau |  hadde |  de  |  hatt |  på |  heimgaren  |  og |  sau |  villand |          |  holda |  på |  med |  videre\n",
    " 0  |    1   |   2  |   3   |  4  |      5      |  6  |   7  |     8    |     9    |    10  |  11 |  12  |   13   \n",
    "             | sub  |             |     sub     |     |      |    sub   |    del   |   sub  |     |      |   sub \n",
    "\n",
    " \n",
    "sau |  hadde |  de  |  hatt |  på |  heimgaren  |  og |  sau |  villand | holda |  på |  med |  videre\n",
    " 0  |    1   |   2  |   3   |  4  |      5      |  6  |   7  |     8    |  9    |  10 |  11  |    12   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, (0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14)]\n",
      "[('sub', 2, 2), ('sub', 5, 5), ('sub', 8, 8), ('delete', 9, 8), ('sub', 10, 9), ('sub', 13, 12)]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "dist.compute_weighted_alignment()\n",
    "candidate_paths = dist.backtrace_alignment_paths()\n",
    "print(list(reversed(deepcopy(candidate_paths[0]))))\n",
    "print(dist.create_editops(candidate_paths[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sub', 0, 0), ('delete', 2, 2), ('sub', 3, 2), ('sub', 4, 3)]\n",
      "{0: 'S', 2: ' ', 3: 'S', 4: 'S'}\n",
      " frå  ||  neste  ||  veke  ||   av   ||  vart  ||  altså  || \n",
      " fra  ||  neste  ||        ||  veka  ||  var   ||  altså  || \n",
      "  S   ||         ||   D    ||   S    ||   S    ||         || \n",
      "\n",
      "[0, (0, 0), (1, 1), (2, 2), (3, 3), (3, 4), (4, 5), (5, 6)]\n",
      "[('sub', 0, 0), ('sub', 2, 2), ('delete', 3, 2), ('sub', 4, 3)]\n",
      "{0: 'S', 2: 'S', 3: ' ', 4: 'S'}\n",
      " frå  ||  neste  ||  veke  ||  av  ||  vart  ||  altså  || \n",
      " fra  ||  neste  ||  veka  ||      ||  var   ||  altså  || \n",
      "  S   ||         ||   S    ||  D   ||   S    ||         || \n"
     ]
    }
   ],
   "source": [
    "from match_em.distances import distance\n",
    "from match_em.alignments import get_alignment_words, print_alignment_words\n",
    "from copy import deepcopy\n",
    "\n",
    "ref = 'frå neste veke av vart altså'\n",
    "hyp = 'fra neste veka var altså'\n",
    "\n",
    "dist3 = distance(ref, hyp)\n",
    "\n",
    "dist3.compute_weighted_alignment()\n",
    "candidate_paths = dist3.backtrace_alignment_paths()\n",
    "\n",
    "print([(op if op[0] != 'replace' else ('sub', op[1], op[2])) for op in dist3.get_levenshtein_editops() ])\n",
    "ct_1, ic_1, ref_1, hyp_1 = get_alignment_words(ref, hyp, dist3.get_levenshtein_editops())\n",
    "print(ic_1)\n",
    "word_align = print_alignment_words(ref_1, hyp_1, index_changes=ic_1, only_print_subs=False, do_print=True, print_char_alignments=False)\n",
    "\n",
    "print()\n",
    "\n",
    "print(list(reversed(deepcopy(candidate_paths[0]))))\n",
    "print(dist3.create_editops(candidate_paths[0]))\n",
    "ct_2, ic_2, ref_2, hyp_2 = get_alignment_words(ref, hyp, dist3.create_editops(candidate_paths[0]))\n",
    "print(ic_2)\n",
    "word_align = print_alignment_words(ref_2, hyp_2, index_changes=ic_2, only_print_subs=False, do_print=True, print_char_alignments=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sub', 2, 2), ('sub', 5, 5), ('delete', 8, 8), ('sub', 9, 8), ('sub', 10, 9), ('sub', 13, 12)]\n",
      "{2: 'S', 5: 'S', 8: ' ', 9: 'S', 10: 'S', 13: 'S'}\n",
      " sau  ||  hadde  ||   d | e | i    ||  hatt  ||  på  ||   h | e | i | m | g | a | r | d | e | n    ||  og  ||  sau  ||   v | i | l | l | e    ||     |\n",
      " sau  ||  hadde  ||   d | e |      ||  hatt  ||  på  ||   h | e | i | m | g | a | r |   | e | n    ||  og  ||  sau  ||     |   |   |   |      ||   v |\n",
      "      ||         ||     |   | D    ||        ||      ||     |   |   |   |   |   |   | D |   |      ||      ||       ||   D | D | D | D | D    ||   I |\n",
      "\n",
      "   |   | h | a | n |      ||   h | a | l | d | a    ||  på  ||  med  ||   v | i | d | a | r | e    ||\n",
      " i | l | l | a | n | d    ||   h | o | l | d | a    ||  på  ||  med  ||   v | i | d | e | r | e    ||\n",
      " I | I | S |   |   | I    ||     | S |   |   |      ||      ||       ||     |   |   | S |   |      ||\n",
      "\n",
      "\n",
      "[0, (0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14)]\n",
      "[('sub', 2, 2), ('sub', 5, 5), ('sub', 8, 8), ('delete', 9, 8), ('sub', 10, 9), ('sub', 13, 12)]\n",
      "{2: 'S', 5: 'S', 8: 'S', 9: ' ', 10: 'S', 13: 'S'}\n",
      " sau  ||  hadde  ||   d | e | i    ||  hatt  ||  på  ||   h | e | i | m | g | a | r | d | e | n    ||  og  ||  sau  ||   v | i | l | l |   |   | e    \n",
      " sau  ||  hadde  ||   d | e |      ||  hatt  ||  på  ||   h | e | i | m | g | a | r |   | e | n    ||  og  ||  sau  ||   v | i | l | l | a | n | d    \n",
      "      ||         ||     |   | D    ||        ||      ||     |   |   |   |   |   |   | D |   |      ||      ||       ||     |   |   |   | I | I | S    \n",
      "\n",
      "||   h | a | n    ||   h | a | l | d | a    ||  på  ||  med  ||   v | i | d | a | r | e    ||\n",
      "||     |   |      ||   h | o | l | d | a    ||  på  ||  med  ||   v | i | d | e | r | e    ||\n",
      "||   D | D | D    ||     | S |   |   |      ||      ||       ||     |   |   | S |   |      ||\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from match_em.distances import distance\n",
    "from match_em.alignments import get_alignment_words, print_alignment_words\n",
    "from copy import deepcopy\n",
    "\n",
    "ref = 'sau hadde dei hatt på heimgarden og sau ville han halda på med vidare'\n",
    "hyp = 'sau hadde de hatt på heimgaren og sau villand holda på med videre'\n",
    "\n",
    "dist3 = distance(ref, hyp)\n",
    "\n",
    "dist3.compute_weighted_alignment()\n",
    "candidate_paths = dist3.backtrace_alignment_paths()\n",
    "\n",
    "print([(op if op[0] != 'replace' else ('sub', op[1], op[2])) for op in dist3.get_levenshtein_editops() ])\n",
    "ct_1, ic_1, ref_1, hyp_1 = get_alignment_words(ref, hyp, dist3.get_levenshtein_editops())\n",
    "print(ic_1)\n",
    "word_align = print_alignment_words(ref_1, hyp_1, index_changes=ic_1, only_print_subs=False, do_print=True, print_char_alignments=True, max_width=150)\n",
    "\n",
    "print()\n",
    "\n",
    "print(list(reversed(deepcopy(candidate_paths[0]))))\n",
    "print(dist3.create_editops(candidate_paths[0]))\n",
    "ct_2, ic_2, ref_2, hyp_2 = get_alignment_words(ref, hyp, dist3.create_editops(candidate_paths[0]))\n",
    "print(ic_2)\n",
    "word_align = print_alignment_words(ref_2, hyp_2, index_changes=ic_2, only_print_subs=False, do_print=True, print_char_alignments=True, max_width=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making sure insertions don't make things weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sub', 0, 0), ('insert', 2, 2), ('sub', 2, 3), ('sub', 3, 4)]\n",
      "{0: 'S', 2: ' ', 3: 'S', 4: 'S'}\n",
      " fra  ||  neste  ||        ||  veka  ||  var   ||  altså  || \n",
      " frå  ||  neste  ||  veke  ||   av   ||  vart  ||  altså  || \n",
      "  S   ||         ||   I    ||   S    ||   S    ||         || \n",
      "\n",
      "[0, (0, 0), (1, 1), (2, 2), (3, 3), (4, 3), (5, 4), (6, 5)]\n",
      "[('sub', 0, 0), ('sub', 2, 2), ('insert', 2, 3), ('sub', 3, 4)]\n",
      "{0: 'S', 2: 'S', 3: ' ', 4: 'S'}\n",
      " fra  ||  neste  ||  veka  ||      ||  var   ||  altså  || \n",
      " frå  ||  neste  ||  veke  ||  av  ||  vart  ||  altså  || \n",
      "  S   ||         ||   S    ||  I   ||   S    ||         || \n"
     ]
    }
   ],
   "source": [
    "from match_em.distances import distance\n",
    "from match_em.alignments import get_alignment_words, print_alignment_words\n",
    "from copy import deepcopy\n",
    "\n",
    "hyp = 'frå neste veke av vart altså'\n",
    "ref = 'fra neste veka var altså'\n",
    "\n",
    "dist3 = distance(ref, hyp)\n",
    "\n",
    "dist3.compute_weighted_alignment()\n",
    "candidate_paths = dist3.backtrace_alignment_paths()\n",
    "\n",
    "print([(op if op[0] != 'replace' else ('sub', op[1], op[2])) for op in dist3.get_levenshtein_editops() ])\n",
    "ct_1, ic_1, ref_1, hyp_1 = get_alignment_words(ref, hyp, dist3.get_levenshtein_editops())\n",
    "print(ic_1)\n",
    "word_align = print_alignment_words(ref_1, hyp_1, index_changes=ic_1, only_print_subs=False, do_print=True, print_char_alignments=False)\n",
    "\n",
    "print()\n",
    "\n",
    "print(list(reversed(deepcopy(candidate_paths[0]))))\n",
    "print(dist3.create_editops(candidate_paths[0]))\n",
    "ct_2, ic_2, ref_2, hyp_2 = get_alignment_words(ref, hyp, dist3.create_editops(candidate_paths[0]))\n",
    "print(ic_2)\n",
    "word_align = print_alignment_words(ref_2, hyp_2, index_changes=ic_2, only_print_subs=False, do_print=True, print_char_alignments=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sub', 2, 2), ('sub', 5, 5), ('insert', 8, 8), ('sub', 8, 9), ('sub', 9, 10), ('sub', 12, 13)]\n",
      "{2: 'S', 5: 'S', 8: ' ', 9: 'S', 10: 'S', 13: 'S'}\n",
      " sau  ||  hadde  ||   d | e |      ||  hatt  ||  på  ||   h | e | i | m | g | a | r |   | e | n    ||  og  ||  sau  ||     |   |   |   |      ||   v |\n",
      " sau  ||  hadde  ||   d | e | i    ||  hatt  ||  på  ||   h | e | i | m | g | a | r | d | e | n    ||  og  ||  sau  ||   v | i | l | l | e    ||     |\n",
      "      ||         ||     |   | I    ||        ||      ||     |   |   |   |   |   |   | I |   |      ||      ||       ||   I | I | I | I | I    ||   D |\n",
      "\n",
      " i | l | l | a | n | d    ||   h | o | l | d | a    ||  på  ||  med  ||   v | i | d | e | r | e    ||\n",
      "   |   | h | a | n |      ||   h | a | l | d | a    ||  på  ||  med  ||   v | i | d | a | r | e    ||\n",
      " D | D | S |   |   | D    ||     | S |   |   |      ||      ||       ||     |   |   | S |   |      ||\n",
      "\n",
      "\n",
      "[0, (0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9), (10, 9), (11, 10), (12, 11), (13, 12), (14, 13)]\n",
      "[('sub', 2, 2), ('sub', 5, 5), ('sub', 8, 8), ('insert', 8, 9), ('sub', 9, 10), ('sub', 12, 13)]\n",
      "{2: 'S', 5: 'S', 8: 'S', 9: ' ', 10: 'S', 13: 'S'}\n",
      " sau  ||  hadde  ||   d | e |      ||  hatt  ||  på  ||   h | e | i | m | g | a | r |   | e | n    ||  og  ||  sau  ||   v | i | l | l | a | n | d    \n",
      " sau  ||  hadde  ||   d | e | i    ||  hatt  ||  på  ||   h | e | i | m | g | a | r | d | e | n    ||  og  ||  sau  ||   v | i | l | l |   |   | e    \n",
      "      ||         ||     |   | I    ||        ||      ||     |   |   |   |   |   |   | I |   |      ||      ||       ||     |   |   |   | D | D | S    \n",
      "\n",
      "||     |   |      ||   h | o | l | d | a    ||  på  ||  med  ||   v | i | d | e | r | e    ||\n",
      "||   h | a | n    ||   h | a | l | d | a    ||  på  ||  med  ||   v | i | d | a | r | e    ||\n",
      "||   I | I | I    ||     | S |   |   |      ||      ||       ||     |   |   | S |   |      ||\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from match_em.distances import distance\n",
    "from match_em.alignments import get_alignment_words, print_alignment_words\n",
    "from copy import deepcopy\n",
    "\n",
    "hyp = 'sau hadde dei hatt på heimgarden og sau ville han halda på med vidare'\n",
    "ref = 'sau hadde de hatt på heimgaren og sau villand holda på med videre'\n",
    "\n",
    "dist3 = distance(ref, hyp)\n",
    "\n",
    "dist3.compute_weighted_alignment()\n",
    "candidate_paths = dist3.backtrace_alignment_paths()\n",
    "\n",
    "print([(op if op[0] != 'replace' else ('sub', op[1], op[2])) for op in dist3.get_levenshtein_editops() ])\n",
    "ct_1, ic_1, ref_1, hyp_1 = get_alignment_words(ref, hyp, dist3.get_levenshtein_editops())\n",
    "print(ic_1)\n",
    "word_align = print_alignment_words(ref_1, hyp_1, index_changes=ic_1, only_print_subs=False, do_print=True, print_char_alignments=True, max_width=150)\n",
    "\n",
    "print()\n",
    "\n",
    "print(list(reversed(deepcopy(candidate_paths[0]))))\n",
    "print(dist3.create_editops(candidate_paths[0]))\n",
    "ct_2, ic_2, ref_2, hyp_2 = get_alignment_words(ref, hyp, dist3.create_editops(candidate_paths[0]))\n",
    "print(ic_2)\n",
    "word_align = print_alignment_words(ref_2, hyp_2, index_changes=ic_2, only_print_subs=False, do_print=True, print_char_alignments=True, max_width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('delete', 0, 0), ('sub', 1, 0), ('sub', 9, 8)]\n",
      "{0: ' ', 1: 'S', 9: 'S'}\n",
      "  g | j | e | n | f | e | r | d    ||     |   |   |   |   |   |   |   | s | a | l | g | e | t    ||  er  ||  rekordstort  ||  sier  ||  mona  ||  ek  \n",
      "    |   |   |   |   |   |   |      ||   j | a | n | f | e | r | d | s | s | a | l | g | e | t    ||  er  ||  rekordstort  ||  sier  ||  mona  ||  ek  \n",
      "  D | D | D | D | D | D | D | D    ||   I | I | I | I | I | I | I | I |   |   |   |   |   |      ||      ||               ||        ||        ||      \n",
      "\n",
      "||  informasjonsansvarlig  ||  i  ||   a | s | c | h | e | h | o | u | g    ||\n",
      "||  informasjonsansvarlig  ||  i  ||   a | s |   | k | e | h | a | u | g    ||\n",
      "||                         ||     ||     |   | D | S |   |   | S |   |      ||\n",
      "\n",
      "\n",
      "[0, (0, 0), (1, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10)]\n",
      "[('sub', 0, 0), ('delete', 1, 0), ('sub', 9, 8)]\n",
      "{0: 'S', 1: ' ', 9: 'S'}\n",
      "  g | j | e | n | f | e | r | d |   |   |   |   |   |   |      ||   s | a | l | g | e | t    ||  er  ||  rekordstort  ||  sier  ||  mona  ||  ek  ||  \n",
      "    | j | a | n | f | e | r | d | s | s | a | l | g | e | t    ||     |   |   |   |   |      ||  er  ||  rekordstort  ||  sier  ||  mona  ||  ek  ||  \n",
      "  D |   | S |   |   |   |   |   | I | I | I | I | I | I | I    ||   D | D | D | D | D | D    ||      ||               ||        ||        ||      ||  \n",
      "\n",
      "informasjonsansvarlig  ||  i  ||   a | s | c | h | e | h | o | u | g    ||\n",
      "informasjonsansvarlig  ||  i  ||   a | s |   | k | e | h | a | u | g    ||\n",
      "                       ||     ||     |   | D | S |   |   | S |   |      ||\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from match_em.distances import distance\n",
    "from match_em.alignments import get_alignment_words, print_alignment_words\n",
    "from copy import deepcopy\n",
    "\n",
    "ref = 'gjenferd salget er rekordstort sier mona ek informasjonsansvarlig i aschehoug'\n",
    "hyp = 'janferdssalget er rekordstort sier mona ek informasjonsansvarlig i askehaug'\n",
    "\n",
    "dist3 = distance(ref, hyp)\n",
    "\n",
    "print([(op if op[0] != 'replace' else ('sub', op[1], op[2])) for op in dist3.get_levenshtein_editops() ])\n",
    "ct_1, ic_1, ref_1, hyp_1 = get_alignment_words(ref, hyp, dist3.get_levenshtein_editops())\n",
    "print(ic_1)\n",
    "word_align = print_alignment_words(ref_1, hyp_1, index_changes=ic_1, only_print_subs=False, do_print=True, print_char_alignments=True, max_width=150)\n",
    "\n",
    "print()\n",
    "\n",
    "dist3.compute_weighted_alignment()\n",
    "candidate_paths = dist3.backtrace_alignment_paths()\n",
    "\n",
    "print(list(reversed(deepcopy(candidate_paths[0]))))\n",
    "print(dist3.create_editops(candidate_paths[0]))\n",
    "ct_2, ic_2, ref_2, hyp_2 = get_alignment_words(ref, hyp, dist3.create_editops(candidate_paths[0]))\n",
    "print(ic_2)\n",
    "word_align = print_alignment_words(ref_2, hyp_2, index_changes=ic_2, only_print_subs=False, do_print=True, print_char_alignments=True, max_width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('delete', 0, 0), ('sub', 1, 0), ('delete', 3, 2)]\n",
      "{0: ' ', 1: 'S', 3: ' '}\n",
      "  i    ||     | f | ø | l | g | e    ||  yr  ||   d | o | t    ||  no  ||  kommer  ||  regnet  ||  fra  ||  øst  || \n",
      "       ||   i | f | ø | l | g | e    ||  yr  ||     |   |      ||  no  ||  kommer  ||  regnet  ||  fra  ||  øst  || \n",
      "  D    ||   I |   |   |   |   |      ||      ||   D | D | D    ||      ||          ||          ||       ||       || \n",
      "\n",
      "[0, (0, 1), (1, 2), (2, 3), (2, 4), (3, 5), (4, 6), (5, 7), (6, 8), (7, 9)]\n",
      "[('delete', 0, -1), ('sub', 1, 0), ('delete', 3, 1)]\n",
      "{0: ' ', 1: 'S', 3: ' '}\n",
      "  i    ||     | f | ø | l | g | e    ||  yr  ||   d | o | t    ||  no  ||  kommer  ||  regnet  ||  fra  ||  øst  || \n",
      "       ||   i | f | ø | l | g | e    ||  yr  ||     |   |      ||  no  ||  kommer  ||  regnet  ||  fra  ||  øst  || \n",
      "  D    ||   I |   |   |   |   |      ||      ||   D | D | D    ||      ||          ||          ||       ||       || \n"
     ]
    }
   ],
   "source": [
    "ref = 'i følge yr dot no kommer regnet fra øst'\n",
    "hyp = 'ifølge yr no kommer regnet fra øst'\n",
    "\n",
    "from match_em.distances import distance\n",
    "from match_em.alignments import get_alignment_words, print_alignment_words\n",
    "from copy import deepcopy\n",
    "\n",
    "dist3 = distance(ref, hyp)\n",
    "\n",
    "print([(op if op[0] != 'replace' else ('sub', op[1], op[2])) for op in dist3.get_levenshtein_editops() ])\n",
    "ct_1, ic_1, ref_1, hyp_1 = get_alignment_words(ref, hyp, dist3.get_levenshtein_editops())\n",
    "print(ic_1)\n",
    "word_align = print_alignment_words(ref_1, hyp_1, index_changes=ic_1, only_print_subs=False, do_print=True, print_char_alignments=True, max_width=150)\n",
    "\n",
    "print()\n",
    "\n",
    "dist3.compute_weighted_alignment()\n",
    "candidate_paths = dist3.backtrace_alignment_paths()\n",
    "\n",
    "print(list(reversed(deepcopy(candidate_paths[0]))))\n",
    "print(dist3.create_editops(candidate_paths[0]))\n",
    "ct_2, ic_2, ref_2, hyp_2 = get_alignment_words(ref, hyp, dist3.create_editops(candidate_paths[0]))\n",
    "print(ic_2)\n",
    "word_align = print_alignment_words(ref_2, hyp_2, index_changes=ic_2, only_print_subs=False, do_print=True, print_char_alignments=True, max_width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('insert', 1, 1), ('delete', 5, 6), ('delete', 6, 6), ('sub', 7, 6), ('sub', 8, 7), ('sub', 15, 14), ('sub', 16, 15), ('sub', 17, 16), ('sub', 18, 17), ('sub', 20, 19)]\n",
      "{1: ' ', 6: ' ', 7: ' ', 8: 'S', 9: 'S', 16: 'S', 17: 'S', 18: 'S', 19: 'S', 21: 'S'}\n",
      " men  ||     |   |      ||  etter  ||  hvert  ||  så  ||  ble  ||   j | e | g    ||   v | a | n | t    ||   t | i | l    ||     |   |   | d | e | t   \n",
      " men  ||   e | e | e    ||  etter  ||  hvert  ||  så  ||  ble  ||     |   |      ||     |   |   |      ||   d | e | t    ||   v | a | n | l | i | g   \n",
      "      ||   I | I | I    ||         ||         ||      ||       ||   D | D | D    ||   D | D | D | D    ||   S | S | S    ||   I | I | I | S | S | S   \n",
      "\n",
      " ||  og  ||  ble  ||  vant  ||  til  ||  å  ||  være  ||   s | a | m | m | e | n    ||     | m | e | d    ||   d | y | r | a    ||   n | e | d | i    \n",
      " ||  og  ||  ble  ||  vant  ||  til  ||  å  ||  være  ||   s | a | m | m | e |      ||   d | y | r | a    ||     | n | e | d    ||     |   |   | i    \n",
      " ||      ||       ||        ||       ||     ||        ||     |   |   |   |   | D    ||   I | S | S | S    ||   D | S | S | S    ||   D | D | D |      \n",
      "\n",
      "||  laben  ||   d | e | r    ||  og  ||  det  ||  var  ||  egentlig  ||  ganske  ||  trivelig  ||\n",
      "||  laben  ||     | e | r    ||  og  ||  det  ||  var  ||  egentlig  ||  ganske  ||  trivelig  ||\n",
      "||         ||   D |   |      ||      ||       ||       ||            ||          ||            ||\n",
      "\n",
      "\n",
      "[0, (0, 0), (1, 1), (2, 1), (3, 2), (4, 3), (5, 4), (6, 5), (7, 6), (8, 7), (8, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (15, 17), (16, 18), (17, 19), (18, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (24, 25), (25, 26), (26, 27)]\n",
      "[('insert', 0, 1), ('sub', 5, 6), ('sub', 6, 7), ('delete', 7, 7), ('delete', 8, 7), ('sub', 15, 14), ('delete', 16, 14), ('sub', 18, 16), ('insert', 18, 17), ('sub', 20, 19)]\n",
      "{1: ' ', 6: 'S', 7: 'S', 8: ' ', 9: ' ', 16: 'S', 17: ' ', 19: 'S', 20: ' ', 22: 'S'}\n",
      " men  ||     |   |      ||  etter  ||  hvert  ||  så  ||  ble  ||   j | e | g    ||   v | a | n |   |   | t    ||   t | i | l    ||   d | e | t    || \n",
      " men  ||   e | e | e    ||  etter  ||  hvert  ||  så  ||  ble  ||   d | e | t    ||   v | a | n | l | i | g    ||     |   |      ||     |   |      || \n",
      "      ||   I | I | I    ||         ||         ||      ||       ||   S |   | S    ||     |   |   | I | I | S    ||   D | D | D    ||   D | D | D    || \n",
      "\n",
      " og  ||  ble  ||  vant  ||  til  ||  å  ||  være  ||   s | a | m | m | e | n    ||   m | e | d    ||  dyra  ||   n | e | d | i    ||        ||  laben \n",
      " og  ||  ble  ||  vant  ||  til  ||  å  ||  være  ||   s | a | m | m | e |      ||     |   |      ||  dyra  ||   n | e | d |      ||   i    ||  laben \n",
      "     ||       ||        ||       ||     ||        ||     |   |   |   |   | D    ||   D | D | D    ||        ||     |   |   | D    ||   I    ||        \n",
      "\n",
      " ||   d | e | r    ||  og  ||  det  ||  var  ||  egentlig  ||  ganske  ||  trivelig  ||\n",
      " ||     | e | r    ||  og  ||  det  ||  var  ||  egentlig  ||  ganske  ||  trivelig  ||\n",
      " ||   D |   |      ||      ||       ||       ||            ||          ||            ||\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ref = 'men etter hvert så ble jeg vant til det og ble vant til å være sammen med dyra nedi laben der og det var egentlig ganske trivelig'\n",
    "hyp = 'men eee etter hvert så ble det vanlig og ble vant til å være samme dyra ned i laben er og det var egentlig ganske trivelig'\n",
    "\n",
    "# ref = 'men     etter hvert så ble jeg vant   til det og ble vant til å være sammen med dyra nedi  laben der og det var egentlig ganske trivelig'\n",
    "# hyp = 'men eee etter hvert så ble det vanlig         og ble vant til å være samme      dyra ned i laben er  og det var egentlig ganske trivelig'\n",
    "\n",
    "from match_em.distances import distance\n",
    "from match_em.alignments import get_alignment_words, print_alignment_words\n",
    "from copy import deepcopy\n",
    "\n",
    "dist3 = distance(ref, hyp)\n",
    "\n",
    "print([(op if op[0] != 'replace' else ('sub', op[1], op[2])) for op in dist3.get_levenshtein_editops() ])\n",
    "ct_1, ic_1, ref_1, hyp_1 = get_alignment_words(ref, hyp, dist3.get_levenshtein_editops())\n",
    "print(ic_1)\n",
    "word_align = print_alignment_words(ref_1, hyp_1, index_changes=ic_1, only_print_subs=False, do_print=True, print_char_alignments=True, max_width=150)\n",
    "\n",
    "print()\n",
    "\n",
    "dist3.compute_weighted_alignment()\n",
    "candidate_paths = dist3.backtrace_alignment_paths()\n",
    "\n",
    "print(list(reversed(deepcopy(candidate_paths[0]))))\n",
    "print(dist3.create_editops(candidate_paths[0]))\n",
    "ct_2, ic_2, ref_2, hyp_2 = get_alignment_words(ref, hyp, dist3.create_editops(candidate_paths[0]))\n",
    "print(ic_2)\n",
    "word_align = print_alignment_words(ref_2, hyp_2, index_changes=ic_2, only_print_subs=False, do_print=True, print_char_alignments=True, max_width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manalysis\u001b[49m\u001b[38;5;241m.\u001b[39mcompute_mistakes(ref, hyp, distance_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted_manual\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'analysis' is not defined"
     ]
    }
   ],
   "source": [
    "analysis.compute_mistakes(ref, hyp, distance_method='weighted_manual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, (0, 0), (1, 1), (2, 1), (3, 2), (4, 3), (5, 4), (6, 5), (7, 6), (8, 7), (8, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (15, 17), (16, 18), (17, 19), (18, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (24, 25), (25, 26), (26, 27)]\n",
      "[('insert', 0, 1), ('sub', 5, 6), ('sub', 6, 7), ('delete', 7, 7), ('delete', 8, 7), ('sub', 15, 14), ('delete', 16, 14), ('sub', 18, 16), ('insert', 18, 17), ('sub', 20, 19)]\n",
      "{1: ' ', 6: 'S', 7: 'S', 8: ' ', 9: ' ', 16: 'S', 17: ' ', 19: 'S', 20: ' ', 22: 'S'}\n",
      " men  ||     |   |      ||  etter  ||  hvert  ||  så  ||  ble  ||   j | e | g    ||   v | a | n |   |   | t    ||   l | i | g    ||   d | e | t    || \n",
      " men  ||   e | e | e    ||  etter  ||  hvert  ||  så  ||  ble  ||   d | e | t    ||   v | a | n | l | i | g    ||     |   |      ||     |   |      || \n",
      "      ||   I | I | I    ||         ||         ||      ||       ||   S |   | S    ||     |   |   | I | I | S    ||   D | D | D    ||   D | D | D    || \n",
      "\n",
      " og  ||  ble  ||  vant  ||  til  ||  å  ||  være  ||   s | a | m | m | e | n    ||   m | e | d    ||  dyra  ||   n | e | d | i    ||        ||  laben \n",
      " og  ||  ble  ||  vant  ||  til  ||  å  ||  være  ||   s | a | m | m | e |      ||     |   |      ||  dyra  ||   n | e | d |      ||   i    ||  laben \n",
      "     ||       ||        ||       ||     ||        ||     |   |   |   |   | D    ||   D | D | D    ||        ||     |   |   | D    ||   I    ||        \n",
      "\n",
      " ||   d | e | r    ||  og  ||  det  ||  var  ||  egentlig  ||  ganske  ||  trivelig  ||\n",
      " ||     | e | r    ||  og  ||  det  ||  var  ||  egentlig  ||  ganske  ||  trivelig  ||\n",
      " ||   D |   |      ||      ||       ||       ||            ||          ||            ||\n",
      "\n",
      "\n",
      "{1: ' ', 6: 'S', 7: 'S', 8: ' ', 15: 'S', 16: ' ', 18: 'S', 20: 'S'}\n",
      "Comp created:  1\n",
      "Comp broke:  1\n",
      " men  ||     |   |      ||  etter  ||  hvert  ||  så  ||  ble  ||   j | e | g    ||   v | a | n | t |   | l | i | g    ||   d | e | t    ||  og  ||  ble  ||  vant  ||  til  ||  å  ||  være  ||   s | a | m | m | e | n    ||   m | e | d    ||  dyra  ||\n",
      " men  ||   e | e | e    ||  etter  ||  hvert  ||  så  ||  ble  ||   d | e | t    ||   v | a | n |   |   | l | i | g    ||     |   |      ||  og  ||  ble  ||  vant  ||  til  ||  å  ||  være  ||   s | a | m | m | e |      ||     |   |      ||  dyra  ||\n",
      "      ||   I | I | I    ||         ||         ||      ||       ||   S |   | S    ||     |   |   | D | D |   |   |      ||   D | D | D    ||      ||       ||        ||       ||     ||        ||     |   |   |   |   | D    ||   D | D | D    ||        ||\n",
      "\n",
      "   n | e | d |   | i    ||  laben  ||   d | e | r    ||  og  ||  det  ||  var  ||  egentlig  ||  ganske  ||  trivelig  ||\n",
      "   n | e | d |   | i    ||  laben  ||     | e | r    ||  og  ||  det  ||  var  ||  egentlig  ||  ganske  ||  trivelig  ||\n",
      "     |   |   | I |      ||         ||   D |   |      ||      ||       ||       ||            ||          ||            ||\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(' men  ||     |   |      ||  etter  ||  hvert  ||  så  ||  ble  ||   j | e | g    ||   v | a | n | t |   | l | i | g    ||   d | e | t    ||  og  ||  ble  ||  vant  ||  til  ||  å  ||  være  ||   s | a | m | m | e | n    ||   m | e | d    ||  dyra  ||   n | e | d |   | i    ||  laben  ||   d | e | r    ||  og  ||  det  ||  var  ||  egentlig  ||  ganske  ||  trivelig  || ',\n",
       " ' men  ||   e | e | e    ||  etter  ||  hvert  ||  så  ||  ble  ||   d | e | t    ||   v | a | n |   |   | l | i | g    ||     |   |      ||  og  ||  ble  ||  vant  ||  til  ||  å  ||  være  ||   s | a | m | m | e |      ||     |   |      ||  dyra  ||   n | e | d |   | i    ||  laben  ||     | e | r    ||  og  ||  det  ||  var  ||  egentlig  ||  ganske  ||  trivelig  || ',\n",
       " '      ||   I | I | I    ||         ||         ||      ||       ||   S |   | S    ||     |   |   | D | D |   |   |      ||   D | D | D    ||      ||       ||        ||       ||     ||        ||     |   |   |   |   | D    ||   D | D | D    ||        ||     |   |   | I |      ||         ||   D |   |      ||      ||       ||       ||            ||          ||            || ')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = 'men etter hvert så ble jeg vant lig det og ble vant til å være sammen med dyra nedi laben der og det var egentlig ganske trivelig'\n",
    "# ref = 'men etter hvert så ble jeg vant til det og ble vant til å være sammen med dyra nedi laben der og det var egentlig ganske trivelig'\n",
    "hyp = 'men eee etter hvert så ble det vanlig og ble vant til å være samme dyra ned i laben er og det var egentlig ganske trivelig'\n",
    "\n",
    "# ref = 'men     etter hvert så ble jeg vant   til det og ble vant til å være sammen med dyra nedi  laben der og det var egentlig ganske trivelig'\n",
    "# hyp = 'men eee etter hvert så ble det vanlig         og ble vant til å være samme      dyra ned i laben er  og det var egentlig ganske trivelig'\n",
    "\n",
    "from match_em.distances import distance\n",
    "from match_em.alignments import get_alignment_words, print_alignment_words\n",
    "from match_em.alignments import check_word_compounding\n",
    "from copy import deepcopy\n",
    "\n",
    "dist4 = distance(ref, hyp)\n",
    "\n",
    "dist4.compute_weighted_alignment()\n",
    "candidate_paths = dist4.backtrace_alignment_paths()\n",
    "ops4 = dist4.create_editops(candidate_paths[0]) \n",
    "\n",
    "print(list(reversed(deepcopy(candidate_paths[0]))))\n",
    "print(ops4)\n",
    "ct_4, ic_4, ref_4, hyp_4 = get_alignment_words(ref, hyp, ops4)\n",
    "print(ic_4)\n",
    "word_align = print_alignment_words(ref_4, hyp_4, index_changes=ic_4, only_print_subs=False, do_print=True, print_char_alignments=True, max_width=150)\n",
    "\n",
    "print()\n",
    "\n",
    "ct_5, ic_5, ref_5, hyp_5, cc_5, cb_5 = check_word_compounding(ct_4, ic_4, ref_4, hyp_4)\n",
    "print(ic_5)\n",
    "print(\"Comp created: \", cc_5)\n",
    "print(\"Comp broke: \", cb_5)\n",
    "word_align = print_alignment_words(ref_5, hyp_5, index_changes=ic_5, only_print_subs=False, do_print=True, print_char_alignments=True)\n",
    "word_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--- UNK_ID (WER: 29.63, compounds created: 1, compounds broken up: 1)---\\n men  ||     |   |      ||  etter  ||  hvert  ||  så  ||  ble  ||   j | e | g    ||   v | a | n | t |   | l | i | g    ||   d | e | t    ||  og  ||  ble  ||  vant  ||  til  ||  å  ||  være  ||   s | a | m | m | e | n    ||   m | e | d    ||  dyra  ||   n | e | d |   | i    ||  laben  ||   d | e | r    ||  og  ||  det  ||  var  ||  egentlig  ||  ganske  ||  trivelig  || \\n men  ||   e | e | e    ||  etter  ||  hvert  ||  så  ||  ble  ||   d | e | t    ||   v | a | n |   |   | l | i | g    ||     |   |      ||  og  ||  ble  ||  vant  ||  til  ||  å  ||  være  ||   s | a | m | m | e |      ||     |   |      ||  dyra  ||   n | e | d |   | i    ||  laben  ||     | e | r    ||  og  ||  det  ||  var  ||  egentlig  ||  ganske  ||  trivelig  || \\n      ||   I | I | I    ||         ||         ||      ||       ||   S |   | S    ||     |   |   | D | D |   |   |      ||   D | D | D    ||      ||       ||        ||       ||     ||        ||     |   |   |   |   | D    ||   D | D | D    ||        ||     |   |   | I |      ||         ||   D |   |      ||      ||       ||       ||            ||          ||            || \\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.compute_mistakes(ref, hyp, distance_method='weighted_manual')['final_print']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " her  ||  i  ||   byen    ||  har  ||  jeg  ||  stort  ||  sett  ||       ||  etter  ||  studietilværelsen  ||  jobba  ||  som  ||  lærer  || \n",
      "      ||     ||  rybyend  ||  har  ||       ||  stort  ||  sett  ||  eee  ||  etter  ||  studietilværelsen  ||  jobba  ||  som  ||  lærer  || \n",
      "  D   ||  D  ||     S     ||       ||   D   ||         ||        ||   I   ||         ||                     ||         ||       ||         || \n"
     ]
    }
   ],
   "source": [
    "ref = 'her i byen har jeg stort sett etter studietilværelsen jobba som lærer'\n",
    "hyp = 'rybyend har stort sett eee etter studietilværelsen jobba som lærer'\n",
    "\n",
    "# ref = 'her i  byen    har jeg stort sett     etter studietilværelsen jobba som lærer'\n",
    "# hyp = '      rybyend  har     stort sett eee etter studietilværelsen jobba som lærer'\n",
    "\n",
    "from match_em.distances import distance\n",
    "from match_em.alignments import get_alignment_words, print_alignment_words\n",
    "from match_em.alignments import check_word_compounding\n",
    "from copy import deepcopy\n",
    "\n",
    "dist4 = distance(ref, hyp)\n",
    "\n",
    "dist4.compute_weighted_alignment()\n",
    "candidate_paths = dist4.backtrace_alignment_paths()\n",
    "ops4 = dist4.create_editops(candidate_paths[0]) \n",
    "\n",
    "ct_4, ic_4, ref_4, hyp_4 = get_alignment_words(ref, hyp, ops4)\n",
    "\n",
    "ct_5, ic_5, ref_5, hyp_5, cc_5, cb_5 = check_word_compounding(ct_4, ic_4, ref_4, hyp_4)\n",
    "word_align = print_alignment_words(ref_5, hyp_5, index_changes=ic_5, only_print_subs=False, do_print=True, print_char_alignments=False, max_width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, (0, 0), (1, 1), (2, 1), (3, 2), (4, 3), (5, 4), (6, 5), (7, 6), (8, 7), (8, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (15, 16), (15, 17), (16, 18), (17, 19), (18, 19), (19, 20), (20, 21), (21, 22), (22, 23), (23, 24), (24, 25), (25, 26), (26, 27)]\n",
      "[('insert', 0, 1), ('sub', 5, 6), ('sub', 6, 7), ('delete', 7, 7), ('delete', 8, 7), ('sub', 15, 14), ('delete', 16, 14), ('sub', 18, 16), ('insert', 18, 17), ('sub', 20, 19)]\n",
      "[(' ', 'eee', 1), ('jeg', 'det', 5), ('vant', 'vanlig', 6), ('til', ' ', 8), ('det', ' ', 9), ('sammen', 'samme', 16), ('med', ' ', 17), ('nedi', 'ned', 19), (' ', 'i', 20), ('der', 'er', 22)]\n",
      "{1: ' ', 6: 'S', 7: 'S', 8: ' ', 9: ' ', 16: 'S', 17: ' ', 19: 'S', 20: ' ', 22: 'S'}\n"
     ]
    }
   ],
   "source": [
    "from match_em.alignments import check_word_compounding\n",
    "\n",
    "print(list(reversed(deepcopy(candidate_paths[0]))))\n",
    "print(dist3.create_editops(candidate_paths[0]))\n",
    "ct_2, ic_2, ref_2, hyp_2 = get_alignment_words(ref, hyp, dist3.create_editops(candidate_paths[0]))\n",
    "print(ct_2)\n",
    "print(ic_2)\n",
    "check_word_compounding(ct_2, ic_2, ref_2, hyp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = 'jeg hadde sett at statoil hadde fått en bot på over hundre millioner kroner'\n",
    "hyp = 'jeg hadde sett at statoil hadde fått en bot på over hundre millioner kroner'\n",
    "\n",
    "# ref = 'her i  byen    har jeg stort sett     etter studietilværelsen jobba som lærer'\n",
    "# hyp = '      rybyend  har     stort sett eee etter studietilværelsen jobba som lærer'\n",
    "\n",
    "from match_em.distances import distance\n",
    "from match_em.alignments import get_alignment_words, print_alignment_words\n",
    "from match_em.alignments import check_word_compounding\n",
    "from copy import deepcopy\n",
    "\n",
    "dist4 = distance(ref, hyp)\n",
    "\n",
    "dist4.compute_weighted_alignment()\n",
    "candidate_paths = dist4.backtrace_alignment_paths()\n",
    "ops4 = dist4.create_editops(candidate_paths[0]) \n",
    "\n",
    "ct_4, ic_4, ref_4, hyp_4 = get_alignment_words(ref, hyp, ops4)\n",
    "\n",
    "ct_5, ic_5, ref_5, hyp_5, cc_5, cb_5 = check_word_compounding(ct_4, ic_4, ref_4, hyp_4)\n",
    "word_align = print_alignment_words(ref_5, hyp_5, index_changes=ic_5, only_print_subs=False, do_print=True, print_char_alignments=False, max_width=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing against sample df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wav2vec_wer import analysis\n",
    "import pandas as pd\n",
    "\n",
    "sample_df = pd.read_csv('sample_df.csv')\n",
    "sample_df.head(3)\n",
    "\n",
    "references = list(sample_df['transcript_cleaned'])\n",
    "hypotheses = list(sample_df['wav2vec'])\n",
    "utt_ids = list(sample_df['utt_id'])\n",
    "\n",
    "results = analysis.compute_mistakes(\n",
    "    references, \n",
    "    hypotheses, \n",
    "    utt_ids, \n",
    "    distance_method='weighted_manual', \n",
    "    print_to_file='word_level_alignments_compounding_insertdelete_manualWeighted.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.199\n",
      "5.694\n"
     ]
    }
   ],
   "source": [
    "print(analysis.wer(results))\n",
    "print(analysis.cer(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_level_errors: 122\n",
      "word_level_references: 604\n",
      "char_level_errors: 210\n",
      "char_level_references: 3688\n",
      "compounds_created: 9\n",
      "compounds_deleted: 8\n",
      "word_miss_pairs\n",
      "[(('frå', 'fra'), 5), (('eit', 'et'), 5), (('høgskulen', 'høgskolen'), 4), (('styre', 'styret'), 3), (('av', ' '), 2), (('vert', 'var'), 2), (('dei', 'de'), 2), (('eg', 'jeg'), 2), (('også', 'og'), 2), (('kalt', 'såkalt'), 2), (('cornflakes', 'kårnfleks'), 2), (('eige', 'eget'), 2), (('zurich', 'syri'), 1), (('reykjavik', 'rekjavik'), 1), (('veke', 'veka'), 1), (('sjølvstendig', 'sjølstendig'), 1), (('eining', 'ening'), 1), (('eige', 'eiet'), 1), (('heimgarden', 'heimgaren'), 1), (('ville', 'villand'), 1), (('han', ' '), 1), (('halda', 'holda'), 1), (('vidare', 'videre'), 1), (('gongene', 'gangene'), 1), (('armane', 'ærmene'), 1), (('hovudet', 'hovedet'), 1), (('sjølv', 'sjøl'), 1), (('leder', 'leider'), 1), (('hanssens', 'hansens'), 1), (('vert', 'vært'), 1), (('hoff', 'hof'), 1), (('aschehoug', 'askehaug'), 1), (('jævla', 'jable'), 1), ((' ', 'og'), 1), (('rødstrømper', 'rødstrumper'), 1), (('kjerringer', 'kjærringer'), 1), (('økter', 'økte'), 1), (('dauding', 'auing'), 1), (('dæhlie', 'dærli'), 1), (('vonde', 'monne'), 1), (('fugleinfluensaen', 'fuglinfluensene'), 1), (('cupcakes', 'kappkaks'), 1), (('playstation', 'pleiestation'), 1), (('rælingen', 'regjeringen'), 1), (('dot', ' '), 1), (('innerloop', 'innalop'), 1), (('undervannsspenning', 'undervannspenning'), 1), (('så', 'og'), 1), (('vert', 'vart'), 1), (('råttent', 'åttende'), 1), (('sjuende', 'syvende'), 1), (('apples', 'appels'), 1), (('lidl', 'lidel'), 1), (('bayern', 'beien'), 1), (('underdogs', 'andredags'), 1), (('middelthon', 'middelta'), 1), (('øygard', 'øykar'), 1), (('særleg', 'særlig'), 1), (('talet', 'tale'), 1), (('utanlandsk', 'utenlandsk'), 1), (('plageånden', 'plagånden'), 1), (('køyrd', 'kjøyr'), 1), (('den', 'en'), 1), (('ikkje', 'ikke'), 1), (('held', 'holder'), 1), (('ein', 'en'), 1), (('svendsen', 'svensen'), 1), (('stakkars', 'stakkersmann'), 1), (('mann', 'han'), 1), (('han', 'har'), 1), (('er', ' '), 1), (('rævkjørt', 'revkjørt'), 1), (('vert', 'ble'), 1), (('høgskulen', 'høgskolene'), 1), (('ei', 'i'), 1), (('eining', 'egning'), 1), (('ørkenrally', 'ørkenrelly'), 1), (('seg', 'sin'), 1), (('nigel', 'naigil'), 1), (('pearson', 'person'), 1), (('vart', 'var'), 1), (('fekk', 'fikk'), 1), (('prege', 'preget'), 1), (('samferdsleutviklinga', 'samferdselutviklinga'), 1)]\n",
      "compound_miss_pairs\n",
      "[(('lo økonom', 'loøkonom'), 1), (('lå ved', 'lover'), 1), (('offside plassert', 'offsidplassert'), 1), (('gjenferd salget', 'janferdssalget'), 1), (('alkoholikerens paranoia', 'alkoholikerenparanoya'), 1), (('passivhus', 'passiv hus'), 1), (('herfra', 'her fra'), 1), (('i følge', 'ifølge'), 1), (('øyhopping', 'øy hopping'), 1), (('itunes politikk', 'ituunspolitikk'), 1), (('actioneventyret', 'aksjen eventyret'), 1), (('veddemålet', 'ved målet'), 1), (('reiulf steen', 'reiulvsteen'), 1), (('tilside', 'til side'), 1), (('forlengst', 'for lengst'), 1), (('newcastle trener', 'jecasteltrener'), 1), (('difor', 'de for'), 1)]\n",
      "char_miss_pairs_word_bound\n",
      "[(('i', ' '), 12), (('e', ' '), 11), (('d', ' '), 9), (('a', 'e'), 9), ((' ', 'e'), 7), ((' ', 't'), 6), (('å', 'a'), 5), (('e', 'a'), 5), (('a', ' '), 5), (('v', ' '), 5), (('t', ' '), 5), (('u', 'o'), 5), (('s', ' '), 5), ((' ', 'j'), 4), (('r', ' '), 4), (('c', 'k'), 4), (('e', 'i'), 4), (('h', ' '), 3), ((' ', 'a'), 3), ((' ', 'n'), 3), (('o', 'a'), 3), (('å', ' '), 3), ((' ', 's'), 3), (('o', ' '), 3), (('c', ' '), 2), (('n', ' '), 2), ((' ', 'å'), 2), ((' ', 'o'), 2), (('e', 'æ'), 2), (('f', ' '), 2), ((' ', 'g'), 2), (('o', 'å'), 2), (('æ', 'e'), 2), (('t', 'e'), 2), (('l', 'e'), 2), (('e', 'l'), 2), (('z', 's'), 1), (('u', 'y'), 1), (('y', ' '), 1), (('g', 'e'), 1), (('e', 't'), 1), (('e', 'd'), 1), (('a', 'o'), 1), (('a', 'æ'), 1), (('u', 'e'), 1), ((' ', 'i'), 1), (('d', 'r'), 1), (('g', ' '), 1), (('h', 'k'), 1), (('i', 'y'), 1), (('æ', 'a'), 1), (('v', 'b'), 1), (('ø', 'u'), 1), (('h', 'r'), 1), (('v', 'm'), 1), (('d', 'n'), 1), ((' ', 'k'), 1), (('c', 'a'), 1), (('u', 'p'), 1), (('a', 'i'), 1), (('y', 'e'), 1), (('l', 'r'), 1), (('r', 'a'), 1), (('s', 'o'), 1), (('å', 'g'), 1), ((' ', 'd'), 1), (('j', 'y'), 1), (('u', 'v'), 1), (('n', 'u'), 1), (('e', 'n'), 1), (('t', 's'), 1), (('i', 'j'), 1), (('o', 'e'), 1), (('y', 'i'), 1), (('u', 'a'), 1), (('e', 'r'), 1), (('r', 'e'), 1), ((' ', 'v'), 1), (('n', 'a'), 1), (('g', 'k'), 1), (('j', ' '), 1), (('e', 'o'), 1), ((' ', 'r'), 1), ((' ', 'm'), 1), (('m', 'h'), 1), (('n', 'r'), 1), (('e', 'b'), 1), (('r', 'l'), 1), (('i', 'g'), 1), (('g', 'n'), 1), (('n', 'j'), 1), (('w', ' '), 1), (('l', ' '), 1), ((' ', 'l'), 1)]\n",
      "char_miss_pairs_word_unbound\n",
      "[(('i', ' '), 12), (('e', ' '), 10), (('d', ' '), 9), (('a', 'e'), 9), (('e', 'a'), 7), ((' ', 'e'), 7), ((' ', 't'), 6), (('å', 'a'), 5), (('a', ' '), 5), (('v', ' '), 5), (('t', ' '), 5), (('u', 'o'), 5), ((' ', 'j'), 4), (('c', 'k'), 4), (('e', 'i'), 4), (('o', 'a'), 3), (('s', ' '), 3), (('r', ' '), 3), (('o', ' '), 3), (('c', ' '), 2), (('h', ' '), 2), ((' ', 'o'), 2), (('e', 'æ'), 2), (('f', ' '), 2), ((' ', 'g'), 2), (('o', 'å'), 2), (('æ', 'e'), 2), (('t', 'e'), 2), (('l', 'e'), 2), (('e', 'l'), 2), (('z', 's'), 1), (('u', 'y'), 1), (('y', ' '), 1), (('g', 'e'), 1), (('e', 't'), 1), ((' ', 'n'), 1), (('h', 'd'), 1), (('n', ' '), 1), (('a', 'o'), 1), (('a', 'æ'), 1), (('u', 'e'), 1), ((' ', 'i'), 1), (('å', ' '), 1), (('d', 'r'), 1), (('g', ' '), 1), ((' ', 's'), 1), (('h', 'k'), 1), (('i', 'y'), 1), (('æ', 'a'), 1), (('v', 'b'), 1), (('ø', 'u'), 1), (('h', 'r'), 1), (('v', 'm'), 1), (('d', 'n'), 1), ((' ', 'k'), 1), (('c', 'a'), 1), (('u', 'p'), 1), (('a', 'i'), 1), (('y', 'e'), 1), (('l', 'r'), 1), (('r', 'a'), 1), (('s', 'o'), 1), (('å', 'g'), 1), ((' ', 'd'), 1), (('j', 'y'), 1), (('u', 'v'), 1), (('n', 'u'), 1), (('e', 'n'), 1), (('t', 's'), 1), (('i', 'j'), 1), (('o', 'e'), 1), (('y', 'i'), 1), (('u', 'a'), 1), (('e', 'r'), 1), (('r', 'e'), 1), ((' ', 'v'), 1), (('n', 'a'), 1), (('g', 'k'), 1), (('j', ' '), 1), (('e', 'o'), 1), ((' ', 'r'), 1), ((' ', 'h'), 1), (('e', 'b'), 1), (('r', 'l'), 1), (('i', 'g'), 1), (('g', 'n'), 1), (('n', 'j'), 1), (('w', ' '), 1), (('l', ' '), 1), ((' ', 'l'), 1), ((' ', 'a'), 1)]\n",
      "known_compounds_achieved\n",
      "[('aksjeselskap', 5), ('treverket', 2), ('plageånden', 1), ('fiskvik', 1), ('statssekretær', 1), ('skyldfølelse', 1), ('rekordstort', 1), ('informasjonsansvarlig', 1), ('målgang', 1), ('sommerparadis', 1), ('spøkelsesby', 1), ('verdensorden', 1), ('fallskjermen', 1), ('kjempegøy', 1), ('autografjegere', 1), ('fagdommerne', 1), ('grisehuset', 1), ('fraukjeller', 1)]\n",
      "known_coupounds_missed\n",
      "[('rødstrømper', 1), ('veddemålet', 1), ('plageånden', 1)]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "for k, v in results.items():\n",
    "    if k == 'final_print':\n",
    "        pass\n",
    "    elif k in ['word_miss_pairs', 'compound_miss_pairs', 'char_miss_pairs_word_bound', 'char_miss_pairs_word_unbound', 'known_compounds_achieved', 'known_coupounds_missed']:\n",
    "        print_list = sorted([(k2, v2) for k2, v2 in results[k].items()], key=itemgetter(1), reverse=True)\n",
    "        print(k)\n",
    "        print(print_list)\n",
    "    else:\n",
    "        print(\"{}: {}\".format(k, v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2., 3.],\n",
       "       [1., 0., 1., 2.],\n",
       "       [2., 1., 1., 2.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import match_em.distances as distances\n",
    "\n",
    "dist = distances.distance('the red cat', 'the rad')\n",
    "dist.compute_unweighted_alignment()\n",
    "dist.distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, (0, 0), (0, 1), (0, 2)],\n",
       " [(0, 0), [(0, 0)], [(1, 1)], [(1, 2)]],\n",
       " [(1, 0), [(1, 1)], [(1, 1)], [(2, 2), (1, 2)]]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.backtrace_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- UNK_ID (WER: 66.67, compounds created: 0, compounds broken up: 0)---\n",
      " the  ||   r | e | d    ||   c | a | t    || \n",
      " the  ||     |   |      ||   r | a | d    || \n",
      "      ||   D | D | D    ||   S |   | S    || \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from match_em.analysis import compute_mistakes\n",
    "\n",
    "print(compute_mistakes('the red cat', 'the rad')['final_print'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- UNK_ID (WER: 50.0, compounds created: 1, compounds broken up: 0)---\n",
      " the  ||   r | e | d |   | c | a | t    || \n",
      " the  ||   r | a | d |   | c | a | t    || \n",
      "      ||     | S |   | D |   |   |      || \n",
      "\n",
      "\n",
      "{'word_level_errors': 1, 'word_level_references': 2, 'char_level_errors': 2, 'char_level_references': 11, 'compounds_created': 1, 'compounds_deleted': 0, 'word_miss_pairs': defaultdict(<class 'int'>, {}), 'compound_miss_pairs': defaultdict(<class 'int'>, {('red cat', 'radcat'): 1}), 'char_miss_pairs_word_bound': defaultdict(<class 'int'>, {('e', 'a'): 1}), 'char_miss_pairs_word_unbound': defaultdict(<class 'int'>, {('e', 'a'): 1}), 'known_compounds_achieved': defaultdict(<class 'int'>, {}), 'known_coupounds_missed': defaultdict(<class 'int'>, {}), 'final_print': '--- UNK_ID (WER: 50.0, compounds created: 1, compounds broken up: 0)---\\n the  ||   r | e | d |   | c | a | t    || \\n the  ||   r | a | d |   | c | a | t    || \\n      ||     | S |   | D |   |   |      || \\n\\n'}\n"
     ]
    }
   ],
   "source": [
    "from match_em.analysis import compute_mistakes\n",
    "\n",
    "result = compute_mistakes('the red cat', 'the radcat', distance_method='weighted_manual')\n",
    "print(result['final_print'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > 1 sub cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wav2vec_wer import analysis\n",
    "import pandas as pd\n",
    "\n",
    "sample_df = pd.read_csv('sample_df.csv')\n",
    "sample_df.head(3)\n",
    "\n",
    "references = list(sample_df['transcript_cleaned'])\n",
    "hypotheses = list(sample_df['wav2vec'])\n",
    "utt_ids = list(sample_df['utt_id'])\n",
    "\n",
    "results_gt_1 = analysis.compute_mistakes(\n",
    "    references, \n",
    "    hypotheses, \n",
    "    utt_ids, \n",
    "    distance_method='weighted_manual', \n",
    "    print_to_file='word_level_alignments_compounding_insertdelete_manualWeighted_greaterThan1Sub.txt',\n",
    "    allow_greater_than_1_sub_cost=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python-Levenshtein check_word_compounding error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wav2vec_wer import analysis\n",
    "\n",
    "# ref = 'da ble  det  nemlig   slik  at  de spilte en en  hjemme   første kampen og så ble det to  to   da på ullevål'\n",
    "# hyp = 'da var      nemisikke              spilt  en ein heime  i første kampen og så ble det to torer    på ulleval'\n",
    "\n",
    "ref = 'da ble det nemlig slik at de spilte en en hjemme første kampen og så ble det to to da på ullevål'\n",
    "hyp = 'da var nemisikke spilt en ein heime i første kampen og så ble det to torer på ulleval'\n",
    "\n",
    "res_1 = analysis.compute_mistakes(ref, hyp, print_to_file='temp_alignment.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " da  ||  ble  ||  det  ||  nemlig  ||  slik  ||     at      ||   de    ||  spilte  ||  en   ||   en    ||  hjemme  ||  første  ||  kampen  ||  og  || \n",
      " da  ||       ||       ||          ||  var   ||  nemisikke  ||  spilt  ||    en    ||  ein  ||  heime  ||    i     ||  første  ||  kampen  ||  og  || \n",
      "     ||   D   ||   D   ||    D     ||   S    ||      S      ||    S    ||    S     ||   S   ||    S    ||    S     ||          ||          ||      || \n",
      "\n",
      " så  ||  ble  ||  det  ||  to  ||  to  ||   da    ||  på  ||  ullevål  ||\n",
      " så  ||  ble  ||  det  ||      ||  to  ||  torer  ||  på  ||  ulleval  ||\n",
      "     ||       ||       ||  D   ||      ||    S    ||      ||     S     ||\n",
      "\n",
      "[('ble', ' ', 1), ('det', ' ', 2), ('nemlig', ' ', 3), ('slik', 'var', 4), ('at', 'nemisikke', 5), ('de', 'spilt', 6), ('spilte', 'en', 7), ('en', 'ein', 8), ('en', 'heime', 9), ('hjemme', 'i', 10), ('to', ' ', 17), ('da', 'torer', 19), ('ullevål', 'ulleval', 21)]\n",
      "{1: ' ', 2: ' ', 3: ' ', 4: 'S', 5: 'S', 6: 'S', 7: 'S', 8: 'S', 9: 'S', 10: 'S', 17: ' ', 19: 'S', 21: 'S'}\n"
     ]
    }
   ],
   "source": [
    "from match_em.alignments import *\n",
    "from match_em.distances import *\n",
    "\n",
    "ref = 'da ble det nemlig slik at de spilte en en hjemme første kampen og så ble det to to da på ullevål'\n",
    "hyp = 'da var nemisikke spilt en ein heime i første kampen og så ble det to torer på ulleval'\n",
    "\n",
    "dist = distance(ref, hyp)\n",
    "computed_editops = dist.get_levenshtein_editops()\n",
    "changes_tuples, index_changes, ref, hyp = get_alignment_words(ref, hyp, computed_editops)\n",
    "word_align = print_alignment_words(ref, hyp, index_changes=index_changes, print_char_alignments=False, do_print=True, max_width=150)\n",
    "print(changes_tuples)\n",
    "print(index_changes)\n",
    "changes_tuples, index_changes, ref, hyp, created_compound, brokeup_compound = check_word_compounding(changes_tuples, index_changes, ref, hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " da  ||   b | l | e    ||  det  ||   n | e | m |   | l | i | g |   |      ||  slik  ||  at  ||  de  ||   s | p | i | l | t | e    ||  en  ||   e |   |\n",
      " da  ||   v | a | r    ||       ||   n | e | m | i | s | i | k | k | e    ||        ||      ||      ||   s | p | i | l | t |      ||  en  ||   e | i |\n",
      "     ||   S | S | S    ||       ||     |   |   |   | S |   | S |   |      ||        ||      ||      ||     |   |   |   |   |      ||      ||     |   |\n",
      "\n",
      " n    ||   h | j | e | m | m | e    ||     ||  første  ||  kampen  ||  og  ||  så  ||  ble  ||  det  ||  to  ||   t | o |   |   |      ||  da  ||  på \n",
      " n    ||   h |   | e | i | m | e    ||  i  ||  første  ||  kampen  ||  og  ||  så  ||  ble  ||  det  ||  to  ||   t | o | r | e | r    ||      ||  på \n",
      "      ||     |   |   | S |   |      ||     ||          ||          ||      ||      ||       ||       ||      ||     |   |   |   |      ||      ||     \n",
      "\n",
      " ||   u | l | l | e | v | å | l    ||\n",
      " ||   u | l | l | e | v | a | l    ||\n",
      " ||     |   |   |   |   | S |      ||\n",
      "\n",
      "[('ble', 'var', 1), ('det', ' ', 2), ('nemlig', 'nemisikke', 3), ('slik', ' ', 4), ('at', ' ', 5), ('de', ' ', 6), ('spilte', 'spilt', 7), ('en', 'ein', 9), ('hjemme', 'heime', 10), (' ', 'i', 11), ('to', 'torer', 19), ('da', ' ', 20), ('ullevål', 'ulleval', 22)]\n",
      "{1: 'S', 2: ' ', 3: 'S', 4: ' ', 5: ' ', 6: ' ', 7: 'S', 9: 'S', 10: 'S', 11: ' ', 19: 'S', 20: ' ', 22: 'S'}\n"
     ]
    }
   ],
   "source": [
    "from match_em.alignments import *\n",
    "from match_em.distances import *\n",
    "\n",
    "ref = 'da ble det nemlig slik at de spilte en en hjemme første kampen og så ble det to to da på ullevål'\n",
    "hyp = 'da var nemisikke spilt en ein heime i første kampen og så ble det to torer på ulleval'\n",
    "\n",
    "dist = distance(ref, hyp)\n",
    "computed_editops = dist.get_weighted_editops()\n",
    "changes_tuples, index_changes, ref, hyp = get_alignment_words(ref, hyp, computed_editops)\n",
    "word_align = print_alignment_words(ref, hyp, index_changes=index_changes, print_char_alignments=True, do_print=True, max_width=150)\n",
    "print(changes_tuples)\n",
    "print(index_changes)\n",
    "changes_tuples, index_changes, ref, hyp, created_compound, brokeup_compound = check_word_compounding(changes_tuples, index_changes, ref, hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_test_pairs_issue_1 = [(l.split(' , ')[0].strip(), l.split(' , ')[1].strip()) for l in '''\n",
    "    den ligger det er en av de østfrisiske øyene , det ligger en av de østfrisiske øyene\n",
    "    da ble det nemlig slik at de spilte en en hjemme første kampen og så ble det to to da på ullevål , da var nemisikke spilt en ein heime i første kampen og så ble det to torer på ulleval\n",
    "    da gifta vi oss i sjømannskirka i palma palma nova , da gifte vi oss i sjømanskirka i palma palmanova\n",
    "    som var en veldig sånn klassisk veldig populært glass glass farge fram til cirka atten atten førti da fabrikken gikk inn , så var det veldig sånn klassisk veldig populært glass glass farge fram til cirka atten attenførti da fabrikken gikk inn\n",
    "    og sånn det er en del kurs og prating om det og og så litt om statistikk som regresjonsanalyse og sånn , og som det er en del kurs og prating om det og også litt om statistikk som regresjonsanalyse og så\n",
    "    så jeg har tenkt å å gå gjennom det som er presentere hva bagama grammatikkene sier om det og og så kontrastere dette herre med et annet konjunktivspråk nemlig moderne fransk , så jeg har tenkt å gå gjennom det som er presentere hva ga gramatikkene sier om det og også kontrastere dette herre med et annet konjunkturforhold nemlig moderne fransk\n",
    "    det s som er stort sett temaet der er v å forklare hva språkkompetanse er for noe , det som sto seg et temaet der å forklare hva språkkompetanse er for noe\n",
    "\n",
    "'''.split('\\n') if l.strip() != '']\n",
    "\n",
    "additional_results = []\n",
    "for test_pair in additional_test_pairs_issue_1:\n",
    "    additional_results.append(\n",
    "        analysis.compute_mistakes(test_pair[0], test_pair[1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sound similarities to refine char alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  p | e | r | l | e | r    || \n",
      "  p | æ |   | l | e | r    || \n",
      "    | S |   |   |   |      || \n",
      "[('perler', 'pæler', 0)]\n",
      "{0: 'S'}\n"
     ]
    }
   ],
   "source": [
    "# from wav2vec_wer.alignments import *\n",
    "# from match_em.distances import *\n",
    "from match_em import distances, alignments\n",
    "\n",
    "ref = 'perler'\n",
    "hyp = 'pæler'\n",
    "\n",
    "dist = distances.distance(ref, hyp)\n",
    "computed_editops = dist.get_weighted_editops()\n",
    "changes_tuples, index_changes, ref, hyp = alignments.get_alignment_words(ref, hyp, computed_editops)\n",
    "word_align = alignments.print_alignment_words(ref, hyp, index_changes=index_changes, print_char_alignments=True, do_print=True, max_width=150)\n",
    "print(changes_tuples)\n",
    "print(index_changes)\n",
    "changes_tuples, index_changes, ref, hyp, created_compound, brokeup_compound = alignments.check_word_compounding(changes_tuples, index_changes, ref, hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "norwegian_letters_by_phoetic_traits = {\n",
    "    'consonants' : {\n",
    "        # order is:\n",
    "        # voiced, \n",
    "        # class (0 - stop, 1 - fricative, 2 - approx, 3 - trill), \n",
    "        # nasal, \n",
    "        # place (0 - bilabial, 1 - labio-dental, 2 - alveolar, 3 - retroflex (not actuall used), 4 - palatal, 5 - velar, 6 - uvular, 7 - glottal), \n",
    "        # lip rounding\n",
    "        'b' : (1, 0, 0, 0, 0),\n",
    "        'c' : [\n",
    "            (0, 0, 0, 5, 0),\n",
    "            (0, 2, 0, 2, 0)\n",
    "        ],\n",
    "        'd' : (1, 0, 0, 2, 0),\n",
    "        'f' : (0, 2, 0, 1, 0),\n",
    "        'g' : (1, 0, 0, 5, 0),\n",
    "        'h' : (0, 2, 0, 7, 0),\n",
    "        'j' : (1, 3, 0, 4, 0),\n",
    "        'k' : (0, 0, 0, 5, 0),\n",
    "        'l' : (1, 3, 0, 2, 0),\n",
    "        'm' : (1, 0, 1, 0, 0),\n",
    "        'n' : (1, 0, 1, 2, 0),\n",
    "        'p' : (0, 0, 0, 2, 0),\n",
    "        'r' : [\n",
    "            (1, 1, 0, 2, 0),\n",
    "            (1, 1, 0, 6, 0)\n",
    "        ],\n",
    "        's' : (0, 2, 0, 2, 0),\n",
    "        't' : (0, 0, 0, 2, 0),\n",
    "        'v' : (1, 2, 0, 1, 0),\n",
    "        'w' : (1, 3, 0, 5, 1),\n",
    "        'z' : (1, 2, 0, 2, 0)\n",
    "    },\n",
    "    'vowels' : {\n",
    "        # we're only going to use front/backness, hight, and rouding. tense/lax seems to be the domain of short vs long vowels and isn't obvious from orthography\n",
    "        # order is height (0 being highest, 2 being lowest), backness (0 being back, 2 being front), and roundess (0 unround and 1 round)\n",
    "        # this height and backness puts 0, 0 at the upper right corner of an IPA vowel chart\n",
    "        'a' : (2, 0, 0),\n",
    "        'e' : (1, 2, 0),\n",
    "        'i' : (0, 2, 0),\n",
    "        'o' : (0, 0, 1),\n",
    "        'u' : (0, 1, 1),\n",
    "        'y' : (0, 2, 1),\n",
    "        'å' : (1, 0, 1),\n",
    "        'æ' : (2, 2, 0),\n",
    "        'ø' : (1, 1, 1)\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'å', 'æ', 'ø']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8284271247461903\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "5.0\n",
      "0.0\n",
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def norwegian_character_distance(v1, v2):\n",
    "    # just euclidian distance\n",
    "    if isinstance(v1[0], tuple):\n",
    "        distances = []\n",
    "        for v1_tup in v1:\n",
    "            distances.append(norwegian_character_distance(v1_tup, v2))\n",
    "        return min(distances)\n",
    "    elif isinstance(v2[0], tuple):\n",
    "        distances = []\n",
    "        for v2_tup in v2:\n",
    "            distances.append(norwegian_character_distance(v1, v2_tup))\n",
    "        return min(distances)\n",
    "    else:\n",
    "        dist = np.sqrt(\n",
    "            # NOTE this is only designed to work with V1 and V2 being the same length\n",
    "            sum([np.square(v1[i] - v2[i]) for i in range(len(v1))])\n",
    "        )\n",
    "        return dist\n",
    "\n",
    "# should be very distant\n",
    "print(norwegian_character_distance(norwegian_letters_by_phoetic_traits['vowels']['i'], norwegian_letters_by_phoetic_traits['vowels']['a'], 'vowel'))\n",
    "# should be very similar\n",
    "print(norwegian_character_distance(norwegian_letters_by_phoetic_traits['vowels']['i'], norwegian_letters_by_phoetic_traits['vowels']['y'], 'vowel'))\n",
    "# should be the same\n",
    "print(norwegian_character_distance(norwegian_letters_by_phoetic_traits['consonants']['t'], norwegian_letters_by_phoetic_traits['consonants']['t'], 'consonant'))\n",
    "# should be very similar\n",
    "print(norwegian_character_distance(norwegian_letters_by_phoetic_traits['consonants']['d'], norwegian_letters_by_phoetic_traits['consonants']['t'], 'consonant'))\n",
    "# should be more distant\n",
    "print(norwegian_character_distance(norwegian_letters_by_phoetic_traits['consonants']['b'], norwegian_letters_by_phoetic_traits['consonants']['g'], 'consonant'))\n",
    "# should be same\n",
    "print(norwegian_character_distance(norwegian_letters_by_phoetic_traits['consonants']['s'], norwegian_letters_by_phoetic_traits['consonants']['c'], 'consonant'))\n",
    "# should be 1.414\n",
    "print(norwegian_character_distance(norwegian_letters_by_phoetic_traits['consonants']['r'], norwegian_letters_by_phoetic_traits['consonants']['c'], 'consonant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, 2, 0, 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def character_sub_score(ref_char, hyp_char):\n",
    "    ref_char_type = 'consonant' if ref_char in norwegian_letters_by_phoetic_traits['consonants'] else 'vowel'\n",
    "    hyp_char_type = 'consonant' if hyp_char in norwegian_letters_by_phoetic_traits['consonants'] else 'vowel'\n",
    "    if ref_char_type == hyp_char_type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.718281828459045"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.38905609893065"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he_llo'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def string_insert(string, index, insert_me):\n",
    "    return string[:index] + insert_me + string[index:]\n",
    "\n",
    "string_insert('hello', 2, '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- UNK_ID (WER: 25.0, compounds created: 1, compounds broken up: 0)---\n",
      " og  ||  så  ||  ble  ||  det  ||  to  ||   t | o |   | d | a |      ||  på  ||   u | l | l | e | v | å | l    || \n",
      " og  ||  så  ||  ble  ||  det  ||  to  ||   t | o |   | r | e | r    ||  på  ||   u | l | l | e | v | a | l    || \n",
      "     ||      ||       ||       ||      ||     |   |   | S | S |      ||      ||     |   |   |   |   | S |      || \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from match_em.analysis import compute_mistakes\n",
    "\n",
    "ref = 'og så ble det to to da på ullevål'\n",
    "hyp = 'og så ble det to torer på ulleval'\n",
    "# specifically this is testing python-Levenshtein get_alignment_word() aka Issue 1\n",
    "results = compute_mistakes(ref, hyp, allow_greater_than_1_sub_cost=True)\n",
    "print(results['final_print'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- UNK_ID (WER: 60.0, compounds created: 1, compounds broken up: 0)---\n",
      "  f | r | å    ||  neste  ||   v | e | k | e |   | a | v    ||   v | a | r | t    ||  altså  || \n",
      "  f | r | a    ||  neste  ||   v | e | k |   |   | a |      ||   v | a | r |      ||  altså  || \n",
      "    |   | S    ||         ||     |   |   | D | D |   | D    ||     |   |   | D    ||         || \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from match_em.analysis import compute_mistakes\n",
    "\n",
    "results = compute_mistakes('frå neste veke av vart altså', 'fra neste veka var altså', distance_method='Levenshtein', )\n",
    "print(results['final_print'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4641016151377544\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_vowel_difference = np.sqrt(np.square(2) + np.square(2) + np.square(2))\n",
    "print(max_vowel_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letter_pair</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a-a</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>æ-æ</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>å-å</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>y-y</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>u-u</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>o-o</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>i-i</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>e-e</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ø-ø</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>u-ø</td>\n",
       "      <td>0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>e-i</td>\n",
       "      <td>0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>å-ø</td>\n",
       "      <td>0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>o-u</td>\n",
       "      <td>0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>u-y</td>\n",
       "      <td>0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>e-æ</td>\n",
       "      <td>0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>i-y</td>\n",
       "      <td>0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>o-å</td>\n",
       "      <td>0.288675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>u-å</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>i-u</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>o-ø</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>e-ø</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>y-ø</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>e-y</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a-å</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>æ-ø</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>i-ø</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>e-u</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a-ø</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a-æ</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>o-y</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>i-æ</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>i-o</td>\n",
       "      <td>0.645497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a-o</td>\n",
       "      <td>0.645497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>y-å</td>\n",
       "      <td>0.645497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>y-æ</td>\n",
       "      <td>0.645497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>e-å</td>\n",
       "      <td>0.645497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a-e</td>\n",
       "      <td>0.645497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a-u</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>i-å</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>u-æ</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>å-æ</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>e-o</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a-i</td>\n",
       "      <td>0.816497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a-y</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>o-æ</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   letter_pair      cost\n",
       "0          a-a  0.000000\n",
       "42         æ-æ  0.000000\n",
       "39         å-å  0.000000\n",
       "35         y-y  0.000000\n",
       "30         u-u  0.000000\n",
       "24         o-o  0.000000\n",
       "17         i-i  0.000000\n",
       "9          e-e  0.000000\n",
       "44         ø-ø  0.000000\n",
       "34         u-ø  0.288675\n",
       "10         e-i  0.288675\n",
       "41         å-ø  0.288675\n",
       "25         o-u  0.288675\n",
       "31         u-y  0.288675\n",
       "15         e-æ  0.288675\n",
       "20         i-y  0.288675\n",
       "27         o-å  0.288675\n",
       "32         u-å  0.408248\n",
       "19         i-u  0.408248\n",
       "29         o-ø  0.408248\n",
       "16         e-ø  0.408248\n",
       "38         y-ø  0.408248\n",
       "13         e-y  0.408248\n",
       "6          a-å  0.408248\n",
       "43         æ-ø  0.500000\n",
       "23         i-ø  0.500000\n",
       "12         e-u  0.500000\n",
       "8          a-ø  0.500000\n",
       "7          a-æ  0.577350\n",
       "26         o-y  0.577350\n",
       "22         i-æ  0.577350\n",
       "18         i-o  0.645497\n",
       "3          a-o  0.645497\n",
       "36         y-å  0.645497\n",
       "37         y-æ  0.645497\n",
       "14         e-å  0.645497\n",
       "1          a-e  0.645497\n",
       "4          a-u  0.707107\n",
       "21         i-å  0.707107\n",
       "33         u-æ  0.707107\n",
       "40         å-æ  0.707107\n",
       "11         e-o  0.707107\n",
       "2          a-i  0.816497\n",
       "5          a-y  0.866025\n",
       "28         o-æ  0.866025"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from match_em.character_distances import get_norwegian_character_sub_cost\n",
    "import pandas as pd\n",
    "\n",
    "with open('/localhome/stipendiater/plparson/match-em/match_em/character_data/character_traits.json', 'r') as open_f:\n",
    "    nor_chars = json.load(open_f)\n",
    "\n",
    "actual_distances = {}\n",
    "\n",
    "for v1 in nor_chars['vowels']:\n",
    "    for v2 in nor_chars['vowels']:\n",
    "        key = '{}-{}'.format(v1, v2)\n",
    "        reverse_key = '{}-{}'.format(v2, v1)\n",
    "        if key not in actual_distances and reverse_key not in actual_distances:\n",
    "            actual_distances[key] = get_norwegian_character_sub_cost(v1, v2)\n",
    "\n",
    "pd.DataFrame([[k, v] for k, v in actual_distances.items()], columns=['letter_pair', 'cost']).sort_values(by='cost')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e98d4e8d717f239dedabd0ca6e2cecb58eeae0ac365073c8c7188790f1ac004"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
